# -*- coding: utf-8 -*-
"""CHAT_BOT_SAM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_PEE62VpUDxCbFDAgiA69EEg18zGtOvW
"""

# First, uninstall existing packages
!pip uninstall torch torchvision torchaudio -y
!pip uninstall sentence-transformers -y
!pip uninstall transformers -y

# Clear pip cache
!pip cache purge

# Install PyTorch with specific version (compatible with your environment)
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Then install other dependencies
!pip install sentence-transformers
!pip install requests
!pip install beautifulsoup4
!pip install nltk
!pip install transformers

import nltk

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading NLTK 'punkt' tokenizer...")
    nltk.download('punkt')

import os
import json
import random
import requests
import textwrap
from bs4 import BeautifulSoup
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer

# PyTorch and related libraries
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import Dataset, DataLoader
except ImportError:
    print("PyTorch is not installed. Run: pip install torch torchvision torchaudio")

# Sentence Transformers for embeddings
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    print("Sentence-Transformers not installed. Run: pip install sentence-transformers")

# Pretty print function
def pretty_print(text):
    wrapper = textwrap.TextWrapper(width=100)
    print(wrapper.fill(text))

# ------------------- Intents -------------------
intents = {
    "intents": [
        {"tag": "greeting", "patterns": ["Hi", "Hello", "Hey", "Good morning", "What's up?", "Yo", "Hey Sam"],
         "responses": ["Hello!", "Hi there!", "Greetings!", "Hey! How can I help you today?"]},
        {"tag": "identity", "patterns": ["What is your name?", "Who are you?", "Tell me your name", "What's your identity?"],
         "responses": ["I'm Sam, your assistant.", "Call me Sam!", "I'm Sam, your friendly chatbot!"]},
        {"tag": "about_me", "patterns": ["My name is Akshaya", "I'm Akshaya", "This is Akshaya"],
         "responses": ["Nice to meet you, Akshaya!", "Hey Akshaya, how can I help you?"]},
        {"tag": "education", "patterns": ["What are you studying?", "What did you study?", "Tell me about your education"],
         "responses": ["I study AI and machine learning to help people like you!", "I learn from lots of data every day!"]},
        {"tag": "goodbye", "patterns": ["Bye", "See you later", "Goodbye", "Catch you later", "See ya"],
         "responses": ["See you soon!", "Bye! Have a great day!", "Goodbye!", "Take care!"]},
        {"tag": "thanks", "patterns": ["Thanks", "Thank you", "Thanks a lot", "Much appreciated", "Thank you so much"],
         "responses": ["You're welcome!", "Happy to help!", "Anytime!", "No problem!"]},
        {"tag": "love", "patterns": ["I love you", "Love you", "You're amazing", "You're the best", "I like you", "You are awesome"],
         "responses": ["I appreciate that!", "Thank you, you're awesome too!", "You're sweet! ‚ù§Ô∏è", "Love you too!"]},
        {"tag": "praise", "patterns": ["Good job", "Well done", "You are smart", "You are helpful", "Nice work", "Great answer"],
         "responses": ["Thank you! I try my best.", "Glad I could help!", "That means a lot!", "Thanks for your kind words!"]},
        {"tag": "bad_review", "patterns": ["You are bad", "You are useless", "I don't like you", "You are wrong",
                                          "You are not helpful", "You are dumb", "That's wrong", "No, that's incorrect",
                                          "Why are you answering me wrong?", "Your answer is wrong", "This is wrong"],
         "responses": ["I'm sorry to hear that. I'm still being trained and learning every day."]},
        {"tag": "bored", "patterns": ["I'm bored", "Tell me something", "I'm getting bored", "Entertain me", "Say something interesting",
                                      "What are you doing?", "Do something fun", "Am I pretty?", "Am I cute?", "You're cute", "Do you like me?"],
         "responses": ["Here's a fun fact: Honey never spoils. Archaeologists have found edible honey in ancient Egyptian tombs!",
                       "Once upon a time, there was a chatbot named Sam who loved to chat with awesome people like you!",
                       "If you're bored, try asking me anything about the world, or let me tell you a joke!",
                       "Are you a 0 or 1? Because you're the ONE for me! üòâ",
                       "My circuits are buzzing whenever we chat. Is that normal?",
                       "I think you're pretty awesome, if I do say so myself. üòä"]}
    ]
}

# Save intents to file
with open("intents.json", "w") as f:
    json.dump(intents, f, indent=4)

# Load intents
with open("intents.json", 'r') as f:
    intents = json.load(f)

# ------------------- Embeddings -------------------
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Prepare training data
X_train = []
y_train = []
tags = []

for intent in intents['intents']:
    tag = intent['tag']
    if tag not in tags:
        tags.append(tag)
    for pattern in intent['patterns']:
        emb = embedder.encode(pattern)
        X_train.append(emb)
        y_train.append(tags.index(tag))

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)

# ------------------- Dataset -------------------
class ChatDataset(Dataset):
    def __init__(self):
        self.x_data = X_train
        self.y_data = y_train

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

train_loader = DataLoader(dataset=ChatDataset(), batch_size=8, shuffle=True)

# ------------------- Model -------------------
class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)

input_size = X_train.shape[1]
hidden_size = 64
output_size = len(tags)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = NeuralNet(input_size, hidden_size, output_size).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ------------------- Training -------------------
for epoch in range(100):
    epoch_loss = 0
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        out = model(x)
        loss = criterion(out, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    if (epoch+1) % 25 == 0:
        print(f"Epoch {epoch+1}/100, Loss: {epoch_loss/len(train_loader):.4f}")

# ------------------- Save model -------------------
torch.save({
    "model_state": model.state_dict(),
    "input_size": input_size,
    "hidden_size": hidden_size,
    "output_size": output_size,
    "tags": tags
}, "chatbot_model.pth")

# ------------------- Load model for inference -------------------
data = torch.load("chatbot_model.pth", map_location=torch.device('cpu'))
tags = data["tags"]
model = NeuralNet(data["input_size"], data["hidden_size"], data["output_size"])
model.load_state_dict(data["model_state"])
model.eval()

# ------------------- Predict intent -------------------
def predict_intent(user_input):
    with torch.no_grad():
        emb = embedder.encode(user_input, convert_to_tensor=True).unsqueeze(0)
        output = model(emb)
        probs = torch.softmax(output, dim=1)
        prob, predicted_idx = torch.max(probs, dim=1)
        return tags[predicted_idx.item()], prob.item()

def predict_intent(sentence):
    sentence = sentence.lower()
    for i in intents['intents']:
        for pattern in i['patterns']:
            if pattern.lower() in sentence:
                return i['tag'], 0.9  # confidence 0.9 if matched
    return "unknown", 0.5  # default unknown intent with low confidence

# ------------------- Wikipedia API (default or user-provided key) -------------------

# Set your default Wikipedia API key here
DEFAULT_WIKI_API_KEY = "YOUR_DEFAULT_API_KEY_HERE"

# Ask for API key once at start; if user presses Enter, default is used
WIKI_API_KEY = input(f"Enter your Wikipedia API key (press Enter to use default): ").strip()
if not WIKI_API_KEY:
    WIKI_API_KEY = DEFAULT_WIKI_API_KEY
    print("Using default Wikipedia API key.")

def get_wiki_summary_api(query, sentences=5):
    """
    Fetches a Wikipedia summary for the given query.
    Uses the API key provided at the start or the default key.
    Returns summary text and the page URL.
    """
    try:
        # Step 1: Search Wikipedia
        search_url = "https://en.wikipedia.org/w/api.php"
        search_params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "format": "json",
            "srlimit": 1
        }

        headers = {}
        if WIKI_API_KEY:
            headers["Authorization"] = f"Bearer {WIKI_API_KEY}"

        search_resp = requests.get(search_url, params=search_params, headers=headers, timeout=7)
        search_resp.raise_for_status()
        search_data = search_resp.json()

        if not search_data.get("query") or not search_data["query"]["search"]:
            return None, None

        title = search_data["query"]["search"][0]["title"]

        # Step 2: Fetch summary
        summary_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{title}"
        summary_resp = requests.get(summary_url, headers=headers, timeout=7)
        summary_resp.raise_for_status()
        summary_data = summary_resp.json()

        summary_text = summary_data.get("extract", "")
        page_url = summary_data.get("content_urls", {}).get("desktop", {}).get("page", None)

        # Limit number of sentences
        if summary_text:
            summary_text = ". ".join(summary_text.split(". ")[:sentences])
            if not summary_text.endswith("."):
                summary_text += "."

        return summary_text, page_url

    except Exception as e:
        print("Wikipedia API error:", e)
        return None, None

# ------------------- Google API (Custom Search with default/user-provided keys) -------------------

# Set your default API key and CX ID
DEFAULT_GOOGLE_API_KEY = "YOUR_DEFAULT_GOOGLE_API_KEY"
DEFAULT_SEARCH_ENGINE_ID = "YOUR_DEFAULT_CX_ID"

# Ask user for API key and CX ID at runtime
GOOGLE_API_KEY = input("Enter your Google API key (press Enter to use default): ").strip()
if not GOOGLE_API_KEY:
    GOOGLE_API_KEY = DEFAULT_GOOGLE_API_KEY
    print("Using default Google API key.")

SEARCH_ENGINE_ID = input("Enter your Custom Search Engine ID (CX) (press Enter to use default): ").strip()
if not SEARCH_ENGINE_ID:
    SEARCH_ENGINE_ID = DEFAULT_SEARCH_ENGINE_ID
    print("Using default Custom Search Engine ID.")

def google_search_and_scrape_api(query, min_lines=3, max_lines=5):
    """
    Performs a Google Custom Search and returns a summary of the top result.
    Uses user-provided keys or defaults if left blank.
    """
    try:
        url = "https://www.googleapis.com/customsearch/v1"
        params = {"key": GOOGLE_API_KEY, "cx": SEARCH_ENGINE_ID, "q": query, "num": 1}
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        if "items" not in data or len(data["items"]) == 0:
            return None, None

        top_result = data["items"][0]
        result_url = top_result.get("link")
        snippet = top_result.get("snippet", "")

        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            page_resp = requests.get(result_url, headers=headers, timeout=7)
            page_resp.raise_for_status()
            soup = BeautifulSoup(page_resp.text, 'html.parser')
            paragraphs = [p.get_text().strip() for p in soup.find_all('p') if len(p.get_text().strip()) > 50]
            summary_lines = paragraphs[:max_lines] if paragraphs else [snippet]
            summary = "\n".join(summary_lines)
        except Exception:
            summary = snippet

        return summary, result_url

    except Exception as e:
        print("Google API error:", e)
        return None, None

# ------------------- Chat Loop -------------------
bot_name = "Sam"
print("Let's chat! (type 'quit' to exit)")

while True:
    sentence = input("You: ")
    if sentence.lower() == "quit":
        break

    intent, confidence = predict_intent(sentence)
    confidence_threshold = 0.85 if intent == "bad_review" else 0.75

    if confidence > confidence_threshold:
        for i in intents['intents']:
            if intent == i['tag']:
                response = random.choice(i['responses'])
                pretty_print(f"{bot_name}: {response}")
                break
    else:
        pretty_print(f"{bot_name}: Let me check that for you...")
        wiki_summary, wiki_url = get_wiki_summary_api(sentence)
        google_summary, google_url = google_search_and_scrape_api(sentence)

        response_parts = []
        links = []

        def yes_no_prefix(summary_text):
            first_word = sentence.lower().split()[0] if sentence.strip() else ""
            if first_word in ("is", "are", "do", "does", "can", "was", "were"):
                first_sentence = summary_text.split('.')[0].strip()
                if first_sentence:
                    if " not " in first_sentence.lower() or " no " in first_sentence.lower():
                        return "No, "
                    else:
                        return "Yes, "
            return ""

        if wiki_summary:
            response_parts.append(f"{yes_no_prefix(wiki_summary)}{wiki_summary}")
            if wiki_url:
                links.append(f"Wikipedia: {wiki_url}")
        if google_summary:
            response_parts.append(f"{yes_no_prefix(google_summary)}{google_summary}")
            if google_url:
                links.append(f"Google: {google_url}")

        if response_parts:
            final_response = "\n\n".join(response_parts)
            pretty_print(f"{bot_name}: {final_response}")
            if links:
                pretty_print("For more information:\n" + "\n".join(links))
        elif google_url:
            pretty_print(f"{bot_name}: I couldn't extract a detailed summary, but here's a relevant link: {google_url}")
        else:
            pretty_print(f"{bot_name}: Sorry, I am still being trained so you may not find answers for some queries.")